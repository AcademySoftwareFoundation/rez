# Rez TSC Meeting Notes - 2023-02-16

:movie_camera::scroll: Recording: https://zoom.us/rec/share/G44Jdfd8beeSsVXNRCV11lBoA9yM6wt_Okgh3Th9IeSD45HAb06Qm-mn3NrSjV8Z.SHPW7TO9jCgqrTYS

## Attendance

* Host: Allan Johns
* Secretary: Jean-Christophe Morin
* TSC Attendees:
  * [x] Allan Johns - NVIDIA
  * [x] Brendan Abel - Walt-Disney Imagineering
  * [x] Jean-Christophe Morin - Freelance
  * [x] Stephen Mackenzie - NVIDIA
  * [x] Thorsten Kaufmann - Mackevision / Accenture
* Other Attendees:
  * Brandon Tebedo (Accenture)
  * Jonas Avrin (Encore VFX)
  * Ruzette Tanyag
  * Sergio Rojas
  * Thomas Trently (Firewalk Studios)

## Agenda
* Agenda Issue: [https://github.com/AcademySoftwareFoundation/rez/issues/1442]
* Thorsten: rez context `apply` issue
* Remote repositories (aka S3, Artifactory, etc) (Jonas, Thomas)
* (Recurring Items): 
  * Open Q&A
    * What are your pain points?
    * Where do you most want to see effort put into the project currently?
    * How do we add the most value (and drive further adoption) with the least effort?
  * Discuss prioritization of efforts, eg:
    * ASWF matters (finalizing governance.md etc);
    * Identifying and triaging quality of life improvements / low hanging fruit;
    * Triaging long-standing PRs;
    * Planning/implementation of longer term features

## Short Version / Decisions / Action Items / Important Links

* Action Items:
  * [Thomas](https://github.com/ttrently): Split synchronuous package caching into separate PR.
  * JC: Go through the PR again.
  * JC: Write a proposal for remote repositories.
* Links:
  * https://github.com/AcademySoftwareFoundation/rez/pull/1453
  * https://github.com/AcademySoftwareFoundation/rez/pull/1380

## Details

### rez context apply

* Thorsten not present at the beginning ot the meeting, so skipped.

### Previous action items

* Anyone: Review and test Windows Shell Pathing PR.
  * Done by JC.
* Anyone: Provide Jean-Christophe with their pip_install_remap config settings.
  * JC received a couple of examples. Mig tneed more, but it's a good start.

### Remote repositories

* Jonas: Company 3 wanted to see if they could use Artifactory to store package artifacts.
  * Because there is on native way to do that right now, they are looking at creating python wheels for all their packages and use rez-pip to install them.
  * Developers previously worked around the problem.
  * JC: Would be curious to know the reasons behind going with wheels instead of something else. JC thinks it's not necessarily recommended to package everything in wheels and then use rez-pip.
  * Jonas: Boils down to having to manage microservices, where rez adds complexity and causes some friction. They wanted to let developers use their normal Python toolsets and not have to worry about Rez.
  * Allan: Rez has a `rez-bundle` which could help. At least it removes the need to have a shared filesystem at runtime.
  * Jonas: That's the workaround they were using previously.
  * JC: Sees rez-bundle as a workaround more than a solution for these cases. rez-bundle has its place, but in this case, it's really a workaround needed because of a lack of tooling.
  * Allan: It could be possible to extend rez-bundle so that the bundle produced has absolutely no dependencies on rez. Right now it will requires rez to use/consume the bundle.
    * We could go as far as either bundling rez into the bundle, or pre-baking environments into the bundle.

* JC: We have a [PR opened](https://github.com/AcademySoftwareFoundation/rez/pull/1453) created by Thomas Trently that adds support to store package payloads in S3 and the package definitions in MongoDB.
  * JC: It's one thing I want to push forward beause it's something I needed in the past and because cloud wprkflows are getting more and more common. Getting rid of the shared filesystem approach will help quite a lot.
  * JC: There is also IT departments that might be happy to not have to deal with payloads on their production NFS servers. Studios might also prefer to have their storage resources used to actually produce images than running software.
  * Allan: Hopefully everyone is aware that package caching exists in rez.
  * Stephen: Cloud bucket repositories is also kind of like a localization or caching feature, because users want to be able to push up to a bucket in one place and let the cloud vendors worry about transferring the data optimally across the world.
    * Stephen: Those same kind of things is what motivated the creation of the `spk` project by SPI and ILM. It can be viewed a little bit like docker where you pull images, which have multiple layers.
    * Stephen: It's cool because it only pull what's needed.
    * Stephe: Once we have this feature, we'll see a bit of need for things lioke package archiving and it's a separation of concerns from a security standpoint.
    * Stephen: There is a lot of use cases that we aren't seing right now that will appear once we have these features in.
  * JC: Good point. To come back to payload caching. Right now it still relies on files being accessible in an uncompressed manner on a shared filesystem. COpies lots of small files over a network storage is quite slow. The cost of coping small files can be too high.
    * JC: Though it could be changed so that we stored payloads in zip files. But we would probably get that for "free" with remote repositories like S3.

* Thomas: About that PR, we put that in a while ago. What Stephen said is on point with what we ran into when we implemented this. We use Amazon S3 as our main repository/storage.
  * Thomas: The PR currently relies on the caching internals to actually make the package payloads available in the environments.
  * Thomas: Had to do some small changes to rez itself to make it possible.
  * Thomas: Those changes will enable a lot of things when it comes to artifact repositories. Right now the caching assumes to payloads are always available in a shared location, so local (shared filesystem) to local (local machine). The change makes it possible to get files from anywhere.
  * Thomas: With that in placev, we can have a generic plugin system that can be used to implement plenty of solutions.
  * Thomas: Right now we just copy files to/from S3 without archving them. But we considered pack files with zip/tar.
  * Thomas: Package definitions are entirely stored in a DB.
  * Thomas: Not sure if security should be built into rez. Too many ways to authenticate to services.
  * Stephen: It seems like we need to give users different handlers for how they want to process data. Same for authentication. It's impossible to serve eveyone well since everyne has different needs.
  * JC: Security will be important to consider. Rez doesn't need to know how to get credentials. We could have plugins that users can implement to fit their needs. I don't think admins will like to see credentials in configuration files, so we need to provide hooks for these things.
  * JC: We don't want to implement something and put the entirely responsibility of security on the users shoulders. Rez also owns part of that responsibility.
  * Stephen: I can also see cases where like rez-bundle could bundle too much, like secrets stored in environment variables etc. That wuold be pretty bad.
  * Thomas: Authentication is abstracted in our environment. Our users already had to authenticate for other needs.
  * Thorsten: We need to support envrionemnts where there is no users to input credentials, like render farms.
  * JC: Repeats that we need a new plugin type that will allow users to decide how credentials will be fetched/accessed. We'll need one or two default plugins thoug, like supporting environment variables, or config files (which is unsafe, but could be an easy way to get something working from a user perspective).
  * Allan: I have some questions about the implementation. How is it implemented in the PR?
    * JC: Credentials are stored ina  S3 specific config file. We need a new type of plugin specifically for payload repositories.
    * Thomas: The database itself is also a different plugin type.
* Allan: There is a bigger picture question here about the development of this project in general: If we take into acccount the modularity proposal, S3 like repositories is the long term goal. That's quite a big engineering task.
  * Allan: I think we don't want to add functionalities to get to this ideal place.
  * JC: Thomas implementation is already quite close to that we want I think. It doesn't require big changes. Package repository and metadata are concepts that already exists in the code base (post edit note: we actually need a new plugin type for package payloads).
  * Allan: I think what studios wants is to actually pull the artifacts and store them in a shared location instead of pulling them straight to the user's machine. And then the caching would localize the payload to the user's system. So sort of like 2 levels of caching.
  * JC: I'm not convinced of that. One main problem that a repository like S3 will solve is to get rid of a shared filesystem.
  * Allan: My point is that if you only got 100$, you'll probably want to only pull the packages from S3 once instead of everytime.
  * Brendan: A lot of workers are remote and connections to shared storage isn't a viable option.
  * Allan: Then I think we should be able to do both.
  * JC: Yeah. Not sure of the use case though.
  * Allan: It's what we would have done at Method, where we had 6-7 studios that are all basically mirrors of each other.
  * Thorsten: We have a usecase ight now for that. We have an off-site studio in Mumbai and they ahve their own Isilon storage server there. What we do is we sync packages there, but it's slow because VPNs and stuff. If I could just the packages in S3 and have rez pull them on the Isilon.
  * Allan: That's exactly the use case Method had.
  * Stephen: I can think of a case where you'd use some kind of micro-service that would do some intelligent decisions about hwich packages to pull from S3, say based on the mecached usage.
  * Allan: Starts' starting to get into the Rez 3 territory, where we'd write everything from scratch :stuck_out_tongue:
  * JC: There is probably a way to do that with `rez-cp` or something like that. If your respository has a webhook system or event system, then you could plugin into it and do actions based on some business logics.
  * Allan: So perhaps the mid term goal is to add a new plugin type to implement custom repositories. It's much simpler from a design point of view.
  * JC: Yep, and then we can build new functionality on top of that later on. It's a solid base to work on. If we wan to later implement the 2 hops kind of workflows described earlier, then we'll already have custom plugins.
  * Allan: yeah, it's something we need anyway.
  * Brendan: The idea of caching something, is it something that's abtract enough? It makes sense from a local file system point of view. But everything is a cache...
  * Thomas: I like the idea of caching. We mgith want to actually change the caching mechanism to make it plugable and flexible.
  * Stephen: Are artifact repositories and caching the same thing just with a different policy or strategy behind them. A cache might have some sort of eviction policy or whatever but a repository just retains all data. what is really the conceptual difference between having an S3 bucket with a bunch of packages in it, having an artifactory repository that's local or something. They're both kind of repositories for the package, and they are both not yet available as part of the runtime. They are somewhere else, you need to pull them. Its kind of the same thing, but maybe just with a different strategy applied to them.
  * Brendan: Or if there's a difference between a local repository, like a filesystem repo, and a cache. Should everything go into the cache or local repo?
  * JC: If you look at other packge managers, they usually have a concept of cache where the package manager will download artifacts into the cache, and then the package amnager will use that cache instead of re-downloading package over and over again.
  * JC: So it could be interesting to have a plugin type for caching.
  * Thorsthen: To me the difference is that a reposito contains the artifacts in a non usable form whereas the cache is pretty much just the packages in a usable form. That's the main different in rez.
  * Thorsten: There is also value in making a distinction between where something is stored and how it is pulled/transferred. Stodios might have third-party tools to handle their transfers and they might want to use these.
* Allan: JC, you said earlier that we can copy packages from one repo to another using `rez-cp`. I want to mention, because I think it affects this. You mentioned before that you actually can copy packages between repositories as long as those functions are implemented plug in. I seem to remember that one approach you could take to implement cashing for any repo type like S3 would be to simply make sure you implement that copy functionality, which is, if I remember correctly, you've to implement functions where you could effectively serialize a package to or from disk. So if you had the S3 repository type and you implemented that function so that a package could be copied to it to a disk backed repository, then the way that you could implement caching for that would be to
just have something that creates a temporary disk package repo and then copies content from S3 into that, and then uses the normal package caching mechanism to get that into the users local cache. That way, the only extra implementation that has to happen is that package copy functionality for any given repo type.
  * Allan: Does that make sense Thomas? Is that something that you considered, or is there a reason why that doesn't make sense?
  * Thomas: We definitely looked into that. But there was issues with package name/location resolution. Some information gets lost along the way, like where the package came from etc.
  * Allan: I'd be great to find out what was the issue and fix it.

* Stephen: It sounds like there is architectural engineering difficulties in terms of understanding what the right solution looks like.
* JC: I think the next steps would be to look at the PR, map the different components and come up with requirements, what we want in the short, middle and longer terms. Once we have that, we'll be in a better to know the current state and make a better, more informed decision.
* JC: I have a good feeling about the PR. I think it's not that far. It probably won't support all use cases, but it's a good base to start from, improve and built on top of.
* JC: I think it's the easiest way to move forward. Make the plugin external to have faster iterations.
* Allan: We'll need to add a synchronuous caching mechanism. Because package will all have to be present locally before entiring into the environment.
* Thomas: Thats what the PR does. We had to make the caching syncronuous.
* Stephen: I think we have an issue open in GH for that already.
* That functionality is needed anyway for packages like DCCs that need to be localized at all cost.
* Stephen: That should go into its own PR since it's much easier to do and it will benefit a lot of people.
* Thorsten: It could be usefull to have a per package atrtibute to say like "must be localized".

* Thanks Thomas and Jonas for sharing all this with us!
